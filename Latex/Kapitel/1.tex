\chapter{Eine Einführung in Maschinelles Lernen}
Da Vorwissen in den Bereichen \gls{KI} und \gls{ML} selbst bei Studierenden der Informatik nicht generell vorausgesetzt werden kann, gibt dieses Kapitel eine kurze Einführung in das Thema. Es wird über die Grundlagen im Bereich des Maschinellen Lernens mit dem Schwerpunkt auf \gls{NL} informiert, die zum Verständnis der Arbeit benötigt werden. Sollte sich der Leser bereits mit dem Thema auseinander gesetzt haben und Begriffe wie "`\gls{Label}"', "`Schichten (Layers)"' und "`Gewichte"' schon bekannt sein, kann dieses Kapitel auch übersprungen werden.

Für große Teile des geschichtlichen Abrisses diente das Buch "`Künstliche Intelligenz, ein moderner Ansatz"' von Stuart Russell und Peter Norvig als Quelle, welches wohl eines der bekanntesten Werke zum Thema \gls{KI} sein dürfte~\cite{Russell.2012}. Generell kann ich dieses Buch allen Interessierten empfehlen, gleichwohl aufgrund des doch sehr großen Umfangs je nach Situation meist nur einzelne Kapitel hilfreich sein werden.

\section{Meilensteine des Maschinellen Lernens} \label{sec:milestones}
In den letzten Jahren (seit ca. 2010) hat sich das Thema \gls{KI} zu einem regelrechten Hype-Thema entwickelt --- und das nicht ganz zu unrecht. Denn gerade durch Anwendungen in den Bereichen Bildverarbeitung, Spracherkennung, sogenannter "`Recommender Systems"' oder auch des automatisierten Fahrens, gab es enorme Fortschritte. Diese machten \gls{KI} für die breite Masse salonfähig und ermöglichten die Entwicklung von Produkten wie Siri, den Skype Translator, Filmempfehlungen auf Netflix oder die Fahrerassistenzsysteme im Tesla Model S, die heute von Millionen von Menschen täglich genutzt werden.

Allen voraus liegt das Hauptaugenmerk vieler Informatiker und Forscher gerade auf den sogenannten "`Künstlichen Neuronalen Netzen"' (Artificial Neural Networks). Manche dieser Neuronalen Netze wurden sogar zu echten Superstars in der Szene, wie zum Beispiel "`AlphaGo"', das von Google entwickelt wurde und 2016 den damaligen Vize-Weltmeister Lee Sedol in vier von fünf Runden im Brettspiel "`Go"' besiegte. Aufgrund der unglaublichen Komplexität\footnote{Ein Go-Brett besteht aus einem Raster von 19 x 19 Plätzen zum Setzen. Für den ersten Zug existieren also 361 Möglichkeiten, für den zweiten 360 und so weiter. Dies ergibt bereits für die ersten drei Züge mehr als 46 Millionen mögliche Spielabläufe.} des Spiels galt der Sieg einer Maschine über einen realen Meister lange Zeit als unmöglich.

Dabei sind die meisten Grundlagen auf diesem Gebiet bereits Jahrzehnte alt. Schon in den 1940er Jahren und somit unmittelbar nach der Erfindung des modernen Computers, begannen die ersten Forscher damit, ein Modell für Künstliche Neuronale Netze zu entwickeln und behaupteten sogar bereits, dass entsprechend definierte Netze auch lernfähig seien. In demselben Artikel, in dem Alan Turing 1950 die Idee des weltbekannten Turing-Tests --- in~\cite{TURING.1950} "`The Imitation Game"' genannt --- veröffentlichte, schrieb er außerdem zum ersten Mal über "`lernende Maschinen"' und philosophierte darüber, wie man einer Maschine beibringen könne, im Imitation Game zu bestehen. In dieser Niederschrift formulierte Turing auch die Grundideen zum heute als "`Reinforcement Learning"' bezeichneten Lernen durch Bestrafung und Belohnung.

1956 war schließlich offiziell das Geburtsjahr der Künstlichen Intelligenz. Am Dartmouth College (Hanover, New Hampshire) veranstalteten McCarthy, Minsky, Shannon und Rochester (allesamt Größen in der Entwicklung der \gls{KI}) ein "`Summer Research Project on Artificial Intelligence"' zusammen mit weiteren Forschern aus ganz Amerika. Hier wurde der Begriff "`Artificial Intelligence"' zum ersten Mal überhaupt benutzt. Ziel des Workshops war es, in zwei Monaten einen signifikanten Fortschritt bei der Entwicklung einer intelligenten Maschine zu erreichen. Dieses Ziel konnte zwar nicht erfüllt werden, jedoch sorgte das Treffen dafür, dass sich die wichtigsten Personen kennenlernten, die in den darauffolgenden 20 Jahren die größten Neuerungen auf diesem Gebiet entwickelten.

Wie auch heute gab es schon einmal in den 1980er Jahren einen großen Boom in der KI-Industrie. Die Investitionen stiegen von einigen Millionen Dollar im Jahr 1980 auf mehrere Milliarden Dollar im Jahr 1988. Viele der \gls{KI}-Firmen konnten ihre Versprechen jedoch nicht halten, weshalb der Markt in den 90er Jahren zusammenbrach. In Folge dessen ging auch die Forschung auf dem Gebiet zurück und es kam zu keinen nennenswerten Erkenntnisgewinnen in den 90er Jahren. Deshalb wird dieses Jahrzehnt auch als "`\gls{KI}-Winter"' bezeichnet.

Wenn alle diese Entwicklungen im Bereich der \gls{KI} aber bereits so lange zurück liegen, warum hat es dann bis heute gedauert, dass es die ersten Anwendungen zum Endkunden schaffen?

Wir leben heute in einer spannenden Zeit, denn einige wichtige Faktoren, die für den Erfolg der \gls{KI} wichtig sind, wurden nahezu zeitgleich verfügbar.

Zum einen stieg die Leistung von Computern seit deren Erfindung stetig an. Für die meisten Berechnungen im Bereich des Maschinellen Lernens wird eine sehr hohe Rechenleistung benötigt. Vor allem die Verwendung von GPUs zur parallelen Berechnung von allgemeinen Aufgaben beschleunigt das Trainieren von neuronalen Netzen um ein Vielfaches. Rechenvorgänge, die noch vor zehn Jahren große Rechencluster für viele Monate beanspruchten, können heute in Stunden, maximal aber wenigen Tagen abgeschlossen werden -- und das auf kompakten Rechnern, die sogar für Privatpersonen erschwinglich sind. Dies gibt den Forschern und Softwareingenieuren die Möglichkeit, schon nach kurzer Zeit ein Feedback zu erhalten, ob die von Ihnen gewählten Ansätze richtig sind und falls nicht, Anpassungen vorzunehmen.

Zum anderen leben wir im Zeitalter von "`Big Data"'. Für \gls{ML} ist es extrem wichtig, dass große Datenmengen zur Verfügung stehen, die für den Trainingsprozess verwendet werden können. Firmen wie Google, Facebook, Apple oder Microsoft (und natürlich vielen anderen) steht ein schier unerschöpflicher Pool an Informationen zur Verfügung. Diese maschinell generierten Daten eignen sich aufgrund ihres großen Umfangs perfekt dazu, neuronale Netze zu trainieren und das wird von diesen Firmen natürlich auch genutzt, um neue Geschäftsideen zu entwickeln und die angebotenen Dienste für ihre Kunden kontinuierlich zu verbessern.

Noch hinzu kam der dritte große Punkt: Die Entwicklung von "`hacks"', welche die bereits erforschten Algorithmen leicht abwandelten, um enorme Einsparungen in der Rechenzeit zu erreichen. So fand man zum Beispiel heraus, dass es in den meisten Fällen ausreicht, nur einen bestimmten Prozentsatz eines neuronalen Netzes zu trainieren, um trotzdem nahezu das gleiche Ergebnis zu erzielen~\cite{Bauckhage2016}.

Natürlich birgt die Entwicklung der Künstlichen Intelligenz auch Gefahren. Diese können ganz real sein, wie der Wegfall tausender Arbeitsplätze~\cite{Russell2015} oder aber spekulativ und in der Zukunft liegend, wie die mögliche Gefährdung der Menschheit durch eine künstliche Superintelligenz~\cite{Barrat2013}. In jedem Fall wird die Weiterentwicklung und Forschung auf dem Gebiet auch die nächsten Jahre ein extrem vielfältiges und spannendes Thema bleiben.

\section{Grundlagen von Neuronalen Netzen}
